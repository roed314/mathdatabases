\documentclass{article}
\usepackage{amsmath,amssymb}
\usepackage{url}
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{colorlinks=true,urlcolor=blue,citecolor=blue,linkcolor=blue}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{colonequals}



%bibtex fix
% \usepackage[style=numeric]{biblatex}
\usepackage{eqparbox}
\usepackage[numbers]{natbib}
\renewcommand*{\bibnumfmt}[1]{\eqparbox[t]{bblnm}{[#1]}}


\usepackage[color=white, linecolor=red, bordercolor=red]{todonotes}
% usage \todo and \todoinline
\newcommand{\todoinline}[1]{\todo[inline]{#1}}

\title{Maintaining Mathematical Databases}
\author{Edgar Costa and David Roe}



\begin{document}
\maketitle

\section{Introduction}

Mathematics has a long history of using computations to aid in forming conjectures and searching for counterexamples.
In the past few decades, computers have taken on a central role, both in performing many calculations and in hosting the results.
As computational and storage capacity has increased, the size of these results has grown to the point where the task of searching and maintaining the data requires specialized knowledge.
We discuss some of the challenges involved, as well as the main tools available to address them.
We focus on one particular case study: the effort to migrate the L-functions and modular forms database from MongoDB to PostgreSQL.\todo{add ref}

We will be primarily interested in datasets consisting of information on mathematical objects, such as specific number fields, graphs or curves, rather than on mathematical statements.
There are many valuable resources focusing on theorems and propositions, from general encyclopedias such as Wikipedia or Encyclopaedia of Mathematics to the preprint server arXiv to domain specific references such as Groupprops. \todo{add citations}
However, the design imperatives for such projects tend to be very different from \todo{continue....} 

Clarify that we're talking about hosting mathematical data, rather than written mathematical content (such as Wikipedia, Mathworld, MathSciNet, arXiv) or something like the Mathematical Genealogy Project.

Audience: mathematicians wanting to create databases, anyone who might be considering switching from MongoDB to PostgreSQL, mathematicians interested in large software projects who might be interested in some of the lessons we've learned from the LMFDB.

\todoinline{mention search by content}

\section{The LMFDB project}
The Langlands program, first formulated by Robert Langlands in the 1960s, is a set of widespread conjectures aimed at understanding and explaining the interconnections between a dizzying array of subfields of mathematics, including number theory, representation theory, algebraic geometry, and harmonic analysis---and in the 21st century its reach continues to expand.
The Langlands program has been called ``one of the biggest ideas to come out of mathematics in the last fifty years'' and ``the Grand Unified Theory of mathematics'' \cite[p. 3]{frenkel}.

To provide compelling visual and computational displays of the Langlands program ``in action'', a database was created called the \emph{$L$-functions and Modular Forms Database (LMFDB)}, available at the website \url{http://www.lmfdb.org/}.
The LMFDB was first conceived in 2007~\cite{cremona-16} and remains the object of a significant amount of ongoing work by over one hundred mathematicians~\cite[\S Acknowledgments]{lmfdb}.

The LMFDB hosts a variety of databases and connects them through the Langlands program.


\section{Other mathematical databases}

\todoinline{What about if we instead of listing all these databases we just use them as examples? for certain things? Or did we have some other goal in mind?}

\subsection{The On-Line Encyclopedia of Integer Sequences}
The The On-Line Encyclopedia of Integer Sequences (OEIS) \url{oeis.org} is an online database of integer sequences, containing over 300,000 sequences and steadily growing.
It records information on integer sequences of interest from a professional mathematicians to general public.
.
Each entry contains the leading terms of the sequence, keywords, mathematical motivations, literature links, and more.
The database is searchable by keyword or by subsequence.
For example, one can search for \href{https://oeis.org/search?q=1,2,0,2,0,5,6,9,0,3,1,5,9,5}{\texttt{1,2,0,2,0,5,6,9,0,3,1,5,9,5}}
to find the sequence \texttt{A002117}, the Decimal expansion of $\zeta(3)$, where $\zeta$ is the Riemann-zeta function.

\subsection{Inverse Symbolic Calculator}
The Inverse Symbolic Calculator (ISC) \url{http://wayback.cecm.sfu.ca/projects/ISC/} is an online database and a set of programs dedicated to the identification of truncated decimal expansion as a closed form representation.

Similarly to OEIS one can also search on \href{http://wayback.cecm.sfu.ca/cgi-bin/isc/lookup?number=1.2020569031595942&lookup_type=simple}{\texttt{1.2020569031595942}}, to find that such decimal expansion seems to be $\zeta(3)$ or
searching on
\href{http://wayback.cecm.sfu.ca/cgi-bin/isc/lookup?number=22.8415439646377&lookup_type=simple}{\texttt{22.841543964637705}} and finding that it is an approximation to $\zeta(3)^{17}$.


\subsection{Encyclopedia of Triangle Centers}

The Encyclopedia of Triangle Centers (ETC) \url{http://faculty.evansville.edu/ck6/encyclopedia/ETC.html} is an online database of triangle centers, this is a point in the plane that is in some sense a center of a triangle akin to the centers other plane figures.
The ETC extends the list of 400 triangle centers originally published in~\cite{kimberling-98} and now covers 32000 triangle centers.

\todo{I'm not sure what else to say about this one, and this one is quite "simple"}


\begin{itemize}
\item Information System on Graph Classes and their Inclusions (\url{graphclasses.org/})
\item Complexity Zoo (\url{https://complexityzoo.uwaterloo.ca/Complexity_Zoo}) - discussion of source of data: computers or humans
\item Digital Library of Mathematical Functions (\url{https://dlmf.nist.gov/})
\item Consequences of the Axiom of Choice (\url{http://www.math.purdue.edu/~hrubin/JeanRubin/Papers/conseq.html})
\item Cantor's Attice (\url{cantorsattic.info})
\item Unipotent Muffin Research Kitchen (\url{http://lie.math.okstate.edu/UMRK/UMRK.html})
\item Atlas of Lie Groups and Representations (\url{http://www.liegroups.org/})
\item Atlas of Finite Group Representations (\url{http://brauer.maths.qmul.ac.uk/Atlas/v3/})
\item FindStat (\url{http://www.findstat.org/})
\item Matroids (\url{https://www.math.ucdavis.edu/~mkoeppe/art/Matroids/})
\item GraphArichive (\url{http://www.graph-archive.org/doku.php}) - example that died
\item House of Graphs (\url{https://hog.grinvin.org/})
\item Table of Knot Invariants (\url{http://www.indiana.edu/~knotinfo/})
\item Knot Atlas (\url{http://katlas.org/})
\item Databases included in GAP/Sage/Magma/etc
\item Predecessors to the LMFDB (Cremona's tables, \url{https://math.la.asu.edu/~jj/localfields/} and other Jones' databases, \url{https://wstein.org/Tables/})
\item Calabi-Yau data \url{http://hep.itp.tuwien.ac.at/~kreuzer/CY/}
\item The K3 Database \url{https://magma.maths.usyd.edu.au/magma/handbook/text/1410}
\item Fano Varieties and Extremal Laurent Polynomials \url{http://coates.ma.ic.ac.uk/fanosearch/}
\end{itemize}

See question of Tim Gowers (https://mathoverflow.net/questions/68442/what-could-be-some-potentially-useful-mathematical-databases) about what databases might be desirable to have.

\section{Nuts and bolts}

When it comes to hosting a mathematical database, there is a wide range of options available.
Until the last several decades, the only option available was to use a print format, for example in a book or an article, and rely on a publisher to distribute its content and to make it available to the rest of scientific community.
Since the publisher takes care of most of the technical issues, this method offers the benefit of simplicity to the author, but distribution in print provides a number of disadvantages compared to digital distribution via the internet.
For instance, the act of publishing through print is a lengthy process, and the data is less immediately accessible than online distribution, putting barriers in the way of the database user.
Furthermore, once the data is published is extremely difficult to address  mistakes, add new data or features.

On the other hand, given that the digital distribution of mathematical data is not taken on by publishers, authors have many more choices to make.
We break down these choices into the following related categories:
\begin{itemize}
  \item back end --- How to encode and manipulate the database in a machine readable format.
  \item front end --- How the user interacts with the database.
  \item hosting --- Where to physically store the database.
\end{itemize}
We will overview some of the possible options in each of these categories

\subsection{Back End}

We organize the options for how to encode the database in a machine readable format in terms of how easy it is to search for content.

\begin{itemize}
  \item human readable files
    ---
     Storing the data set in a human readable format, like a  plain text file or a comma-separated values file, is one of the most straightforward possible solutions.
    Furthermore, it also does not require special software to access the data, very similar to a table in print.
    However, it can be cumbersome and impractical to search or manipulate the data without additional tools.
  \item application dependent files
    ---
    A more advanced solution is to store the dataset in an application dependent format.
    For example, the Small Groups Library available in the GAP computer algebra system. \todo{addref}
    In many cases, such a choice enables direct manipulation of the desired objects.
    For example, using SageMath pickles makes it easy to load the data into an active SageMath session.
    However, the dataset will not likely be efficiently searchable, and most content searches would need to loop over every item in the dataset in a sequential search.
  \item database structure
    ---
    Finally, one may store the dataset in database format, i.e., in a format optimized for searching.
    However, such sophisticated solution will most likely require additional software for users to to manipulate the data.
    There are many types of database available; in this article, we fill focus on two:
    \begin{itemize}
      \item relational database --- a collection of ledger-style tables with a fixed schema, where the structure in each row of is constant.
        This kind of databases have been prevalent for decades and some of their famous instantiations are Oracle, MySQL, PostgreSQL, and SQLite.
      \item document oriented database --- a schema free database, where each object is stored in a single instance in the database, and every stored object can be different from every other.
        MongoDB is one of the most famous instantiations of such paradigm.
    \end{itemize}
    In Section~\ref{sec:MvsP} we will contrast PostgreSQL and MongoDB.
\end{itemize}


\subsection{Front End}

We now go through some of the options for how the user can interact with a database.
\begin{itemize}
  \item direct access
    ---
    The author may decide to provide direct access to the database.
    For example, the author may make the database files available through E-mail or a web page; or
    allow direct read access to a database server.

  \item computer algebra system
    ---
    If the database is provided in a computer algebra system dependent files, these might evolve into being part of algebra system itself.
    For example, Small Groups Library available in GAP or the K3 Database in Magma. \todo{addref}
    This option is extremely convenient for the user, however, this is not suitable for very large databases, as these would increase the size of the computer algebra system in question.

  \item website
    ---
    The user may also interact with the database through a collection of web pages displaying the data.
    These web pages can range widely in complexity.
    For example, one can manually curate each page individually
    These pages can be humanly curate each object page, as in
    \begin{itemize}
      \item Cantorâ€™s Attice \url{http://cantorsattic.info/}
      \item Fano Varieties and Extremeal Laurent Polynomials \url{http://coates.ma.ic.ac.uk/fanosearch/}
    \end{itemize}
    or, automatically generate them as static web pages, as in
    \begin{itemize}
      \item Encyclopedia of Triangle Centers \url{https://faculty.evansville.edu/ck6/encyclopedia/ETC.html}
      \item Graph Classes \url{http://graphclasses.org/index.html}
      \item SyzygyData \url{http://syzygydata.com/}
      \item Table of Knot Invariants \url{http://www.indiana.edu/~knotinfo/}
    \end{itemize}
    Alternatively, one can instead use an web application to generate each page whet tis accessed.
    This is essential for large datasets, as generating and storing a static web page for each object, might be unfeasible.
    LMFDB follows such approach, where all the content is generated on the fly through a dedicated web application based of Flask \todo{addref \url{http://flask.pocoo.org/}}, and only some small amount of the content is humanly curated, definitions and others.
    Other examples of mathematical databases using such approach contain:
    \begin{itemize}
      \item Knot Atlas \url{http://katlas.org}
      \item House of Graphs (\url{https://hog.grinvin.org/})
    \end{itemize}
\end{itemize}

\subsection{Hosting}


For last we discuss some of the options regarding where to physically store the database.

\begin{itemize}
  \item personal hard-drive
    ---
    This is one of the simples options, as it has no initial set up cost,
    however, it severely limits the accessibility of the database to others
  \item personal web page
    ---
    If one already maintains a personal web page, a convenient option is to also use it to host their database.
    However, the scope might be limited, as one does not has full access to the hosting servers.
  \item web framework
    ---
    Another convenient option is to use a web framework service (e.g. Github, Cocalc, Wordpress/wikidot/blogs).
    These options offer a lower initial set up cost with a minimal long term maintenance cost, however, the scope is also limited by the framework.
  \item On a university/personal server
    ---
    This solution provides complete control.
    However, not only, there is a high initial monetary/effort cost of buying the servers and setting them up, one also needs to upkeep the servers for the rest of their life.
    Furthermore, the scope might be limited by some of the initial hardware choices.
  \item using a hosting service
    ---
    Servers in the cloud (e.g. Google, AWS) provides complete control, this has an monetary associated maintenance costs, however, one does not need to worry about mainentance tasks, uptime, and etc.
\end{itemize}






\section{MongoDB vs PostgreSQL}
\label{sec:MvsP}

In 2009, when MongoDB was initially released as an open source project in it presented itself as an exciting new option to consistently store mathematical objects.
As a document-oriented database, where each document is stored independently in a JSON-like schemata, it did not require a schema, a boon when creating new collections of mathematical objects where the data requirements only gradually became clear.
Moreover, Python bindings were available, a language that many computational number theorists are familiar with due to its use in SageMath.
This setup worked well for many years; however, as the LMFDB grew various shortcomings of using a non relational database became more evident.
Over the last year, we have transitioned from MongoDB to PostgreSQL, a popular open source implementation of the SQL language that grew out of earlier projects founded in the 1970s and 80s.
As a relational database, PostgreSQL stores data in a structured way, where the data is organized in tables, and each table has a specified schema, i.e., each row must have the same layout, with the types of columns constant across all rows.

\subsection{Comparing two database systems}

MongoDB and PostgreSQL offer two different database paradigms.
MongoDB, as a document-oriented database, is meant to facilitate the storage of unstructured data, where the fields and types present in each document can vary across a collection.
This flexibility is extremely convenient, but can also easily lead to errors and unnecessary overhead for a structured database.
For example, field names were kept short in MongoDB to save on space usage and multiple typos in the field names were found during the transition to PostgreSQL.
Furthermore, differing layouts across documents requires more complicated processing code.
In contrast, PostgreSQL is a relational database, where fixed schemas enable queries across multiple tables.
Since the LMFDB was already a uniform set of fields in MongoDB (to the extent that an inventory application was written to extract a schema from the data), creating a formal schema was fairly straightforward.
Database-level schema support significantly improves robustness and performance, and PostgreSQL's JSONB type allows for schemaless columns when desired.
One benefit of switching has been that storing the data in PostgreSQL requires substantially less space than in MongoDB.
For example, when converting our biggest MongoDB collection \texttt{Lfunctions.Lfunctions} to the PostgreSQL table \texttt{lfunc\_lfunctions}, we observed a space savings of about 42\%, going from 194GiB to 113GiB.
These savings translate both to monetary savings (we were spending about \$2000 per year in MongoDB paying for storage space) and to query speed if a sequential search is required.

Two main factors account for the smaller storage requirements.
Because the fields in a MongoDB document can vary, the names of these fields must be stored in each document.
For some of the LMFDB's collections, these names accounted for a substantial fraction of the storage requirement.
In addition to the space benefits, the switch to PostgreSQL also relaxes the pressure to minimize the lengths of these names, improving readability.

The second factor contributing to space savings is the reduced use of strings in the data itself.
As a mathematical database, the LMFDB was heavily impacted by MongoDB's lack of support for arbitrary precision integers.
In order to work around this deficiency, a lot of numerical data was stored as strings.
In addition to the storage consequences, various workarounds were required in order to sort and search the data correctly.
The elimination of these hacks have simplified a lot of the supporting code.


While smaller tables help with query performance, indexes provide a more powerful tool for speeding up searches.
Indexes facilitate the location of records with constraints on a specified set of columns by storing additional data.
Having such indexes available is key to provide quick search results for typical queries.
For example, both MongoDB and PostgreSQL support indexes based on binary trees, which work well for totally ordered data such as integers and strings. \todo{add ref}
However, there is another query type that is not well supported by binary trees: given a column representing a set of integers, search for rows that contain a particular set, or are contained in a particular set.
For example, we store the set of ramified primes for number fields, and want to be able to specify specific primes as either ramified or unramified.
GIN indexes in PostgreSQL support such queries efficiently, while they timed out after 30 seconds when running in MongoDB.


The transition to PostgreSQL also led to a significant improvement in indexless queries.
Such queries are important since non-typical queries can be the most interesting mathematically, and since it is impossible to construct all indexes in advance because the LMFDB offers the user a lot of freedom to select search parameters.
When comparing sequential queries (those without indexes), it's important to specify the memory model used by each database.
MongoDB offers two different storage engines: Memory Mapping (MMAP), which uses large amounts of memory in order to put as much data as possible into RAM, and WiredTiger, which balances memory usage with filesystem caching.
In a system with hundreds of gigabytes of memory, MMAP significantly outperforms WiredTiger.
However, in a more constrained memory environment, we found that WiredTiger outperforms MMAP on most queries, while also providing data compression at the disk level.
For these reasons, WiredTiger was the storage engine behind \url{www.LMFDB.org}, and forms the basis for our comparison with PostgreSQL. \todo{Add numerical performance comparisons}

While search performance is key for website usability, the process of uploading data and creating indexes is also important in order to make the development sustainable and to aid in the implementation of new features.
PostgreSQL's \texttt{COPY FROM} command allows for rapid importing of data from plain-text files without a Python intermediary, which is useful when the data is being generated by some other system such as Magma \cite{magma}.
Creating indexes in PostgreSQL is also dramatically faster than in MongoDB. \todo{quantify}
Moreover, PostgreSQL supports transactions,\footnote{MongoDB 4.0 added support for transactions but was released after we switched to PostgreSQL.} allowing for atomic changes to multiple rows in a table.
This feature makes it possible to safely make some modifications in place, rather than creating a copy of the whole table when changes are made.
PostgreSQL also offers numerous open source front-ends and administration tools \cite{pgadmin,adminer} that lower the learning curve for the SQL language.
Database management has been substantially improved by the transition.

In addition to performance improvements, PostgreSQL makes new kinds of queries possible.
Relational databases allow joining tables, returning values from both tables when they share some specified relationship.
The existing schemas, having been designed for MongoDB, do not take advantage of this feature, but new tables on finite groups and on p-adic tori will.
Going forward, the ability to make connections between different kinds of mathematics is one of the core aims of the LMFDB, and a relational database will help us achieve this goal.

%\begin{itemize}
%\item Have to specify a schema in PostgreSQL in advance.  This can be a hassle, but also can prevent errors (e.g. typos in column names, more complicated processing code because data isn't uniform)
%\item MongoDB required more storage space since every document had to include the column names (quantify storage space change; quantify how much we're paying)
%\item Integers limited to 64 bits, so got in the habit of storing many things as strings (which made sorting tricky among other issues)
%\item Array types (not just a jsonb)
%\item MongoDB required more RAM (Wired-Tiger; MMAP). Postgres can be faster with more RAM but works okay with less.
%\item Indexes on lists didn't work as we wanted (e.g. list of ramified primes) - would work for "what are all the fields ramified at 5" but not "what are all the fields ramified at 2 and 3".  Also inefficient (constructs query inefficiently since there are many more fields than primes).  Postgres has multiple index types, including gin which work well for this purpose
%\item Performance improvements, important since we don't know what kind of queries the user will request so it's hard to build all indexes.  Partly because of lower storage, performance also improved even without indexes.  Non-typical queries can be the most interesting mathematically.
%\item Harder to do relational queries in MongoDB; since we ported the table layout relational queries are still not used much in Postgres
%\item Postgres overcame much of MongoDB's advantage with the inclusion of the json and jsonb types.
%%\item MongoDB was faster at counting results.  Work around this by caching counts in a table (possible since data doesn't change frequently)
%\item Useful new utilities (pgAdmin 4)
%\item Fast loading from a file (COPY FROM), and direct data from Magma without a Python intermediary
% Loaded data into mf_hecke_cc in 29032.052 secs from /scratch/mf/cc/mf_hecke_cc.txt
%\item (TODO) Actually compare performance while we still have the mongo instance around.
%\item Transactions (bugs don't cause data reliability problems) and data upload much faster
%\end{itemize}



\subsection{Transition}

Once the decision had been made to switch from MongoDB to PostgreSQL, we faced two main tasks: to port the data and to update the LMFDB codebase to use PostgreSQL.
The majority of the work was done by the second author between March 2018 and August 2018, with assistance from the first author starting in July.

Since PostgreSQL requires the specification of schemas, the initial step of the data migration involved defining the schema for each table.
In practice, most of the tables had a schema already, which were made visible by a custom inventory module within the LMFDB.%
\footnote{The inventory displayed the kinds of documents within each collection, the fields present in each kind of document, the indexes present in each collection, and both machine and human generated information on the types of each field.
It was a very useful resource for constructing schema.}
Unfortunately, the accuracy of these schema varied: some collections had multiple independent kinds of records within the same table, while others had typos in some of the field names.
Moreover, the inventory system itself proved to inaccurately reflect the types of some columns, since it inferred the type from finding a single record.
For example, it concluded that a certain field was a real number, while some documents had complex values.
Eventually we overcame these issues were overcome and created a schema for each PostgreSQL table.

In some cases, there were multiple possible types to choose from for a given column.
PostgreSQL 9.4 added support for a binary json type, which allows for unstructured data to be stored in a column.
It provided an alternative to arrays that could also store an analogue of Python's dictionaries, with strings as keys.
For simplicity, we initially chose to use this jsonb type instead of arrays, since it simplified some of the supporting code.
However, over the past year we have realized that when arrays are possible (namely, when type is uniform across all entries), they provide some important advantages.
Namely, arrays are more space efficient than the jsonb type, and indexes on arrays support more operations than indexes on a jsonb column.
We are gradually transitioning jsonb to array columns where useful.

We also took advantage of the transition to change some data types that had been chosen due to the deficiencies in MongoDB.
For example, many large integers had been stored as strings since MongoDB lacked an arbitrary precision data type.
With the switch to PostgreSQL, we were able to use the numeric type instead, which simplified code used to sort and compute with the results.

Since one of the goals of the switch was improved performance, we made some changes to the schemas to help speed up queries.
In particular, we split some of the largest tables in half, putting columns that were used for searching into a search table and columns that were only used for display into an extras table.
This greatly reduced the size of the search table, improving index efficiency and speeding sequential queries that had to traverse the whole table.
A second optimization was only feasible because the data in the LMFDB changes very rarely, and because the search pages come with a standard order.
Many tables are ordered lexicographically by four or five columns.
We added a primary key column to each table, and for large tables we ordered this primary key by the default sort order for that table.
This feature decreased the size of indexes and improved their performance, since search results could be ordered by a single numeric column rather than a combination of many columns.

With the schemas created and export scripts written, the actual changeover of the data went fairly smoothly.
PostgreSQL provides a COPY FROM command to load data from a text file, which runs very quickly.
After several false starts, exporting 400GB of data to text files took three or four days, copying it between servers on opposite sides of the Atlantic took less than a day and loading it into PostgreSQL took less than a day.

Adapting the LMFDB codebase to use PostgreSQL took longer, and we are still finishing up a few bits of that part of the transition.
The backend code for the LMFDB is written in Python, and the Python bindings for MongoDB and PostgreSQL take a very different approach.
Pymongo offers a high level interface to the database that integrates python data structures.
Queries are performed by constructing dictionaries that specify the values of certain columns, and have special keys for adding more complicated constraints such as inequalities or boolean combinations of conditions.
Psycopg2, on the other hand, is a thin interface that offers the user the ability to execute SQL statements.
Constructing queries using dictionaries worked well as a model for the LMFDB, since the search pages provided many inputs that could be processed independently into a dictionary.
Moreover, we did not want to require LMFDB developers to learn SQL.
We therefore implemented a high level interface to PostgreSQL that translated a dictionary-based query system similar to Pymongo's into SQL SELECT statements.

\section{Benefits}

\subsection{Abstraction}

As the LMFDB has grown, the most common paradigm for adding new features has been to take a functional section of the website, copy it to a new folder, and modify the templates and backend code.
This approach has the benefit of easily producing valid code, since one is iteratively making small changes to an already functional webpage.
However, it leads to large quantities of repeated code, which make fixing bugs and adding features much harder, since the same change must be applied in dozens of locations.
The standard solution is to encapsulate tasks into functions, that can then be used repeatedly and changed at need.
Of course, the LMFDB has used functions since the beginning, but often these functions are not written in a way that allows them to be used in multiple sections of the website.

The task of switching from MongoDB to PostgreSQL was made more difficult by this low level of abstraction, but it also presented an opportunity.
Since the database change necessitated changing a large number of files throughout the project, it offered a perfect time to make additional changes.
We highlight four in particular to illustrate the kinds of encapsulation that accompanied the database change.

First, we centralized many of the scripts used to upload data into a suite of data management tools. \todo{Finish description of data management tools (changing schema, importing new data from Python and from files)}

Each section of the LMFDB has a search page, where users can input constraints they would like their curve, field or group to satisfy.
Some input boxes require an integer or list of integers; others need a label string in a particular format; others a rational number.
The processing code must process the user provided text, raising appropriate errors on invalid input, and transform the set of inputs into a database query.
Prior to the database change, we had already begun the process of encapsulating this task.
Each input box corresponds to a specific field or column in the database, and there is a processing function for each type of input (list of integers, floating point, etc).
As we switched each part of the LMFDB to PostgreSQL, we ensured that it was using these functions.

Once these queries have been constructed, we need to execute the search and pass the results to the Jinja template that actually crafts the webpage seen by the user.
There are various parts of this task that are shared among all sections of the LMFDB.
For example, we have to be able to handle errors in user input, errors or timeouts in the execution of the query, and jump boxes that allow a user to go straight to an object with a particular label.
As part of the switch to PostgreSQL, we created a \emph{search wrapper} that performs all of these jobs in a uniform way, and adopted it across the LMFDB.
In addition to simplifying the code overall, this change made the user experience more uniform and allowed for the creation of new features, which will be described in the next section.

Many sections of the LMFDB had a statistics page prior to the database change.
These pages described the number of objects with certain attributes; for example, a table of how many number fields were available by signature, or a table giving the fraction of elliptic curves with a specified torsion structure.
Along with the database change, we created a class to support the database queries required for these statistics, and a common template to display them nicely.
This change has the benefit that it automates the process for updating these counts when developers add data.
It also make it easier to create these statistics pages for more sections of the LMFDB, and makes it easier to implement new features such as user-requested statistics.

\subsection{New Features}

In addition to intrinsic benefits of using PostgreSQL, the transition has eased the implementation of a number of new features.
While many of these would have been possible using MongoDB, the improved abstraction made implementing them much more manageable.
And some, such as dynamic statistics and verifications, rely on PostgreSQL's improved performance and relational model.

PostgreSQL supports transactions, allowing one to roll back to a functional earlier state if some later action fails.
We have built these transactions into our interface layer, and this change has enabled new paradigms when making data changes.
Rather than having to create a whole new copy of a table when making substantial changes, transactions allow sufficient peace-of-mind that more operations can be performed in-place.

The fact that all interactions with the database pass through our own interface layer allows us to add custom logging.
For example, all queries that take longer than a threshold are recorded, allowing us to focus on creating the right indexes.
We also log high-level actions, such as adding new data to a table or changing a schema, along with a username in order to provide a record of what changes have been made to the data.
These logs help us ensure the quality of our data.

The statistics infrastructure has enabled a new user interface which we refer to as \emph{dynamic statistics}.
Rather than presenting only a pre-selected set of views, the user can specify which variables they are interested in, as well as constraints on the objects.
The system the generates a display describing the possible values of those variables and the number of objects possessing each possible set of values.
For example, a user can create a table to view how the weight and level of modular forms vary among forms with complex multiplication.
We hope that this feature will provide researchers with a new method of interacting with the LMFDB, allowing them to look for large scale patterns in these objects in addition to searching for objects with particular properties.

Many browse pages in the LMFDB already had a link to view a random object.
Because of the new search wrapper abstraction, it was easy to add the ability to return a random object satisfying a search query.
This feature saves time when browsing, since it allows you to more quickly reach the homepage of an object satisfying a particular constraint, such as CM elliptic curve or a weight 1 modular form.

Mathematical data differs from data in most other applications because it has a notion of correctness that can be checked, rather than just providing a record of a real-world measurement.
Some of these verifications are internal, such as checking that the defining polynomial of a number field is irreducible or that the trace of a modular form is zero at each inert prime not dividing the level.
Others rely on connections with other tables in the database, such as checking that invariants match up between an elliptic curve and its corresponding modular form.
The switch to PostgreSQL made it possible to create a framework to write such consistency checks and run them whenever new data is added.

\bibliographystyle{plain}
\bibliography{biblio}

\end{document}
