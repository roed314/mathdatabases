\documentclass{article}
\usepackage{amsmath,amssymb}
\usepackage{url}
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{colorlinks=true,urlcolor=blue,citecolor=blue,linkcolor=blue}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{colonequals}



%bibtex fix
% \usepackage[style=numeric]{biblatex}
\usepackage{eqparbox}
\usepackage[numbers]{natbib}
\renewcommand*{\bibnumfmt}[1]{\eqparbox[t]{bblnm}{[#1]}}


\newcommand{\todo}[1]{\textcolor{red}{#1}}

\title{Maintaining Mathematical Databases}
\author{Edgar Costa and David Roe}



\begin{document}
\maketitle

\section{Introduction}

Clarify that we're talking about hosting mathematical data, rather than written mathematical content (such as Wikipedia, Mathworld, MathSciNet, arXiv) or something like the Mathematical Genealogy Project

\section{The LMFDB project}
The Langlands program, first formulated by Robert Langlands in the 1960s, is a set of widespread conjectures aimed at understanding and explaining the interconnections between a dizzying array of subfields of mathematics, including number theory, representation theory, algebraic geometry, and harmonic analysis---and in the 21st century its reach continues to expand.
The Langlands program has been called ``one of the biggest ideas to come out of mathematics in the last fifty years'' and ``the Grand Unified Theory of mathematics'' \cite[p. 3]{frenkel}.


To provide compelling visual and computational displays of the Langlands program ``in action'', a database was created called the \emph{$L$-functions and Modular Forms Database (LMFDB)}, available at the website \url{http://www.lmfdb.org/}.
The LMFDB was first conceived in 2007~\cite{cremona-16} and remains the object of a significant amount of ongoing work by over one hundred mathematicians \cite[\S Acknowledgments]{lmfdb}.


The LMFDB hosts a variety of databases and connects them through the Langlands program.


\section{Other mathematical databases}

\todo{What about if we instead of listing all these databases we just use them as examples? for certain things? Or did we have some other goal in mind?}

\subsection{The On-Line Encyclopedia of Integer Sequences}
The The On-Line Encyclopedia of Integer Sequences (OEIS) \url{oeis.org} is an online database of integer sequences, containing over 300,000 sequences and steadily growing.
It records information on integer sequences of interest from a professional mathematicians to general public.
.
Each entry contains the leading terms of the sequence, keywords, mathematical motivations, literature links, and more.
The database is searchable by keyword or by subsequence.
For example, one can search for \href{https://oeis.org/search?q=1,2,0,2,0,5,6,9,0,3,1,5,9,5}{\texttt{1,2,0,2,0,5,6,9,0,3,1,5,9,5}}
to find the sequence \texttt{A002117}, the Decimal expansion of $\zeta(3)$, where $\zeta$ is the Riemann-zeta function.

\subsection{Inverse Symbolic Calculator}
The Inverse Symbolic Calculator (ISC) \url{http://wayback.cecm.sfu.ca/projects/ISC/} is an online database and a set of programs dedicated to the identification of truncated decimal expansion as a closed form representation.

Similarly to OEIS one can also search on \href{http://wayback.cecm.sfu.ca/cgi-bin/isc/lookup?number=1.2020569031595942&lookup_type=simple}{\texttt{1.2020569031595942}}, to find that such decimal expansion seems to be $\zeta(3)$ or
searching on
\href{http://wayback.cecm.sfu.ca/cgi-bin/isc/lookup?number=22.8415439646377&lookup_type=simple}{\texttt{22.841543964637705}} and finding that it is an approximation to $\zeta(3)^{17}$.


\subsection{Encyclopedia of Triangle Centers}

The Encyclopedia of Triangle Centers (ETC) \url{http://faculty.evansville.edu/ck6/encyclopedia/ETC.html} is an online database of triangle centers, this is a point in the plane that is in some sense a center of a triangle akin to the centers other plane figures.
The ETC extends the list of 400 triangle centers originally published in \cite{kimberling-98} and now covers 32000 triangle centers.

\todo{I'm not sure what else to say about this one, and this one is quite "simple"}


\begin{itemize}
\item Information System on Graph Classes and their Inclusions (\url{graphclasses.org/})
\item Complexity Zoo (\url{https://complexityzoo.uwaterloo.ca/Complexity_Zoo}) - discussion of source of data: computers or humans
\item Digital Library of Mathematical Functions (\url{https://dlmf.nist.gov/})
\item Consequences of the Axiom of Choice (\url{http://www.math.purdue.edu/~hrubin/JeanRubin/Papers/conseq.html})
\item Cantor's Attice (\url{cantorsattic.info})
\item Unipotent Muffin Research Kitchen (\url{http://lie.math.okstate.edu/UMRK/UMRK.html})
\item Atlas of Lie Groups and Representations (\url{http://www.liegroups.org/})
\item Atlas of Finite Group Representations (\url{http://brauer.maths.qmul.ac.uk/Atlas/v3/})
\item FindStat (\url{http://www.findstat.org/})
\item Matroids (\url{https://www.math.ucdavis.edu/~mkoeppe/art/Matroids/})
\item GraphArichive (\url{http://www.graph-archive.org/doku.php}) - example that died
\item Table of Knot Invariants (\url{http://www.indiana.edu/~knotinfo/})
\item Databases included in GAP/Sage/etc
\item Knot Atlas (\url{http://katlas.org/})
\item Predecessors to the LMFDB (Cremona's tables, \url{https://math.la.asu.edu/~jj/localfields/} and other Jones' databases, \url{https://wstein.org/Tables/})
\item Calabi-Yau data \url{http://hep.itp.tuwien.ac.at/~kreuzer/CY/}
\item The K3 Database \url{https://magma.maths.usyd.edu.au/magma/handbook/text/1410}
\item Fano Varieties and Extremal Laurent Polynomials \url{http://coates.ma.ic.ac.uk/fanosearch/}
\end{itemize}

See question of Tim Gowers (https://mathoverflow.net/questions/68442/what-could-be-some-potentially-useful-mathematical-databases) about what databases might be desirable to have.

\section{Hosting}

\begin{itemize}
\item On a personal webpage (easy for many academics), but limits scope (aren't running your own webserver)
\item Using a web framework (e.g. Github, Cocalc, Wordpress/wikidot/blogs)
\item On a university/personal server.  Complete control, but have to deal with down time, buying new servers, backups, security updates, DNS....
\item Servers in the cloud (e.g. Google, AWS).  Reduces hassle, but costs money.
\end{itemize}

\section{Front End}

\begin{itemize}
\item E-mail me if you want the data
\item HTML front page with downloads (static)
\item Wiki page (dynamic but human generated)
\item within computer algebra system (GAP/Sage/Magma)
\item web application (LMFDB uses Python/Flask/Jinja with javascript/CSS/etc).  Allows for dynamic generation of pages, links, nicer representation of data
\end{itemize}

\section{Back End}

\begin{itemize}
\item static files (plain text, sql-lite, Sage pickles,....)
\item Wiki software
\item document oriented database (e.g. MongoDB: schema free)
\item relational database (e.g. PostgreSQL)
\end{itemize}


\section{MongoDB vs PostgreSQL}

In 2009, when MongoDB was initially released as an open source project in it presented itself as an exciting new option to consistently store mathematical objects.
As a document-oriented database, where each document is stored independently in a JSON-like schemata, it did not require a schema, a boon when creating new collections of mathematical objects where the data requirements only gradually became clear.
Moreover, Python bindings were available, a language that many computational number theorists were familiar with due to its use in SageMath.
It worked well for many years, but as the LMFDB grew various shortcomings of using a non relational database became more problematic.
Over the last year, we have transitioned from MongoDB to PostgreSQL, a popular open source implementation of the SQL language that grew out of earlier projects founded in the 1970s and 80s.
As an relational database, PostgreSQL stores data in structure way in tables with a specified schema: each row must have the same layout, with the types of columns constant across all rows.

\subsection{Comparing two database systems}

The crucial difference between MongoDB and PostgreSQL it is their paradigm.
MongoDB as a document-oriented database is meant to facilitate the storage of unstructured data, where the fields and types present in each document can vary across a collection.
This flexibility is extremely convenient, but can also easily lead to errors and unnecessary overhead for a structured database.
For example, field names were kept short in MongoDB to save on space usage and multiple typos in the field names were found during the transition to PostgreSQL.
Furthermore, differing layouts across documents requires a more complicated processing code.
This overhead was unnecessary as the LMFDB was already using a schema before the transition, to the extent that an inventory application was written to extract the schema from the data in MongoDB.
However, supporting the schema at the level of the database software significantly improves robustness and performance.
Furthermore, PostgreSQL overcame much of MongoDB's advantage of schemeless collections with the inclusion of the JSON and JSONB types.
As a consequence of the paradigm shift, storing the data in PostgreSQL requires substantially less space than in MongoDB.
For example, when converting our biggest MongoDB collection \texttt{Lfunctions.Lfunctions} to the PostgreSQL table \texttt{lfunc\_lfunctions}, we observed a space savings of about 42\%, going from 194GiB to 113GiB.
\todo{mention that in MongoDB we pay about 2k per year on storage space}

Two main factors account for this improvement.
Because the fields in a MongoDB document can vary, the names of these fields must be stored in each document.
For some of the LMFDB's collections, these names accounted for a substantial fraction of the storage requirement.
In addition to the space benefits, the switch to PostgreSQL also relaxes the pressure to minimize the lengths of these names, improving readability.

The second factor contributing to space savings is the reduced use of strings in the data itself.
As a mathematical database, the LMFDB was heavily impacted by MongoDB's limited type support and its indexes \todo{add ref}
For example, the lack of support for arbitrary precision integers.
In order to work around this issue, a lot of numerical data was stored as strings.
In addition to the storage consequences, various workarounds were required in order to sort and search the data correctly.
The elimination of these hacks have simplified a lot of the supporting code.




One of the main tools for improving search performance on databases is the use of indexes.
Indexes facilitate the location of records with constraints on a specified set of columns by storing additional data.
Having such indexes available is key to provide quick searche results for typical queries.
For example, both MongoDB and PostgreSQL support indexes based on binary trees, which work well for totally ordered data such as integers and strings.
However, there is another query type that is not well supported by binary trees.
\todo{wouldn't be better to just explain this with lists, and then give the example of Number fields?}
If $K$ is a number field, the LMFDB stores a list $P_K$ of ramified primes.
We would like to be able to specify a list $Q$ of primes and be able to find all $K$ with $Q \subseteq P_K$, or all $K$ with $Q \supseteq P_K$.
MongoDB executed these searches by narrowing the results one prime at a time \todo{confirm that this is actually what MongoDB is doing}
In contrast, GIN indexes in PostgreSQL support such queries efficiently.






For last, MongoDB offered us two different storage engines Memory Mapping (MMAP) and WiredTiger.
The major difference between these systems is that MMAP automatically tries to (dynamically) use all free memory on the machine, while WiredTiger tries to balance the memory usage with filesystem cache, this is very similar to what PostgreSQL also does.
In a system with enough memory to fit a big chunk of the data set, MMAP significantly outperforms WiredTiger.
However, in a more realist low memory environment system, we found that WiredTiger outperforms MMAP in most queries, while also providing data compression at the disk level.
For these reasons, WiredTiger was the storage engine behind \url{www.LMFDB.org}.
In the transition to PostgreSQL we noticed a significant improvement in indexless queries, which is critical as we don't know a priori what kind of queries the user will request so it's hard to build all indexes. Furthemore, the non-typical queries can be the most interesting mathematically.


Given the maturity of PostgreSQL there are numerous open source front-ends and tools for administering that lower the introduction barrier to the SQL language.

\begin{itemize}
%\item Have to specify a schema in PostgreSQL in advance.  This can be a hassle, but also can prevent errors (e.g. typos in column names, more complicated processing code because data isn't uniform)
%\item MongoDB required more storage space since every document had to include the column names (quantify storage space change; quantify how much we're paying)
%\item Integers limited to 64 bits, so got in the habit of storing many things as strings (which made sorting tricky among other issues)
%\item Array types (not just a jsonb)
%\item MongoDB required more RAM (Wired-Tiger; MMAP). Postgres can be faster with more RAM but works okay with less.
%\item Indexes on lists didn't work as we wanted (e.g. list of ramified primes) - would work for "what are all the fields ramified at 5" but not "what are all the fields ramified at 2 and 3".  Also inefficient (constructs query inefficiently since there are many more fields than primes).  Postgres has multiple index types, including gin which work well for this purpose
%\item Performance improvements, important since we don't know what kind of queries the user will request so it's hard to build all indexes.  Partly because of lower storage, performance also improved even without indexes.  Non-typical queries can be the most interesting mathematically.
\item Harder to do relational queries in MongoDB; since we ported the table layout relational queries are still not used much in Postgres
%\item Postgres overcame much of MongoDB's advantage with the inclusion of the json and jsonb types.
\item MongoDB was faster at counting results.  Work around this by caching counts in a table (possible since data doesn't change frequently)
%\item Useful new utilities (pgAdmin 4)
\item Fast loading from a file (COPY FROM), and direct data from Magma without a Python intermediary
  % Loaded data into mf_hecke_cc in 29032.052 secs from /scratch/mf/cc/mf_hecke_cc.txt
\item (TODO) Actually compare performance while we still have the mongo instance around.
\item Transactions (bugs don't cause data reliability problems) and data upload much faster
\end{itemize}



\subsection{Transition}

\begin{itemize}
\item Exporting data to plain text, readable using PostgreSQL' COPY FROM.
\item Choice of schema.  LMFDB's Inventory was very useful here.  Had to choose between jsonb and arrays; initially chose to just use jsonb for simplicity but more recently have used arrays for smaller storage footprint.  Tried to normalize data to some extent (away from using strings for so much).
\item Data irregularities slowed the process down
\item Added abstraction layers.  psycopg2 is very low level (provides a way to execute SQL commands from Python and get the results back).  Because the lmfdb has almost no queries that cross multiple databases, was able to port MongoDB dictionary queries: implemented a parser that generates simple SQL.  Means that most LMFDB developers don't need to learn SQL.
\item Build data management tools into the interface (changing schema, importing new data from Python and from files).  Replaces parts of scripts.
\item Some optimizations (id ordering, split some tables in half)
\end{itemize}

\section{Benefits}

\subsection{Abstraction}

As the LMFDB has grown, the most common paradigm for adding new features has been to take a functional section of the website, copy it to a new folder, and modify the templates and backend code.
This approach has the benefit of easily producing valid code, since one is iteratively making small changes to an already functional webpage.
However, it leads to large quantities of repeated code, which make fixing bugs and adding features much harder, since the same change must be applied in dozens of locations.
The standard solution is to encapsulate tasks into functions, that can then be used repeatedly and changed at need.
Of course, the LMFDB has used functions since the beginning, but often these functions are not written in a way that allows them to be used in multiple sections of the website.

The task of switching from MongoDB to PostgreSQL was made more difficult by this low level of abstraction, but it also presented an opportunity.
Since the database change necessitated changing a large number of files throughout the project, it offered a perfect time to make additional changes.
We highlight three in particular to illustrate the kinds of encapsulation that accompanied the database change.

Each section of the LMFDB has a search page, where users can input constraints they would like their curve, field or group to satisfy.
Some input boxes require an integer or list of integers; others need a label string in a particular format; others a rational number.
The processing code must process the user provided text, raising appropriate errors on invalid input, and transform the set of inputs into a database query.
Prior to the database change, we had already begun the process of encapsulating this task.
Each input box corresponds to a specific field or column in the database, and there is a processing function for each type of input (list of integers, floating point, etc).
As we switched each part of the LMFDB to PostgreSQL, we ensured that it was using these functions.

Once these queries have been constructed, we need to execute the search and pass the results to the Jinja template that actually crafts the webpage seen by the user.
There are various parts of this task that are shared among all sections of the LMFDB.
For example, we have to be able to handle errors in user input, errors or timeouts in the execution of the query, and jump boxes that allow a user to go straight to an object with a particular label.
As part of the switch to PostgreSQL, we created a \emph{search wrapper} that performs all of these jobs in a uniform way, and adopted it across the LMFDB.
In addition to simplifying the code overall, this change made the user experience more uniform and allowed for the creation of new features, which will be described in the next section.

Many sections of the LMFDB had a statistics page prior to the database change.
These pages described the number of objects with certain attributes; for example, a table of how many number fields were available by signature, or a table giving the fraction of elliptic curves with a specified torsion structure.
Along with the database change, we created a class to support the database queries required for these statistics, and a common template to display them nicely.
This change has the benefit that it automates the process for updating these counts when developers add data.
It also make it easier to create these statistics pages for more sections of the LMFDB, and makes it easier to implement new features such as user-requested statistics.

\subsection{New Features}

In addition to intrinsic benefits of using PostgreSQL, the transition has eased the implementation of a number of new features.
While many of these would have been possible using MongoDB, the improved abstraction made implementing them much more manageable.
And some, such as dynamic statistics and verifications, rely on PostgreSQL's improved performance and relational model.

PostgreSQL supports transactions, allowing one to roll back to a functional earlier state if some later action fails.
We have built these transactions into our interface layer, and this change has enabled new paradigms when making data changes.
Rather than having to create a whole new copy of a table when making substantial changes, transactions allow sufficient peace-of-mind that more operations can be performed in-place.

The fact that all interactions with the database pass through our own interface layer allows us to add custom logging.
For example, all queries that take longer than a threshold are recorded, allowing us to focus on creating the right indexes.
We also log high-level actions, such as adding new data to a table or changing a schema, along with a username in order to provide a record of what changes have been made to the data.
These logs help us ensure the quality of our data.

The statistics infrastructure has enabled a new user interface which we refer to as \emph{dynamic statistics}.
Rather than presenting only a pre-selected set of views, the user can specify which variables they are interested in, as well as constraints on the objects.
The system the generates a display describing the possible values of those variables and the number of objects possessing each possible set of values.
For example, a user can create a table to view how the weight and level of modular forms vary among forms with complex multiplication.
We hope that this feature will provide researchers with a new method of interacting with the LMFDB, allowing them to look for large scale patterns in these objects in addition to searching for objects with particular properties.

Many browse pages in the LMFDB already had a link to view a random object.
Because of the new search wrapper abstraction, it was easy to add the ability to return a random object satisfying a search query.
This feature saves time when browsing, since it allows you to more quickly reach the homepage of an object satisfying a particular constraint, such as CM elliptic curve or a weight 1 modular form.

Mathematical data differs from data in most other applications because it has a notion of correctness that can be checked, rather than just providing a record of a real-world measurement.
Some of these verifications are internal, such as checking that the defining polynomial of a number field is irreducible or that the trace of a modular form is zero at each inert prime not dividing the level.
Others rely on connections with other tables in the database, such as checking that invariants match up between an elliptic curve and its corresponding modular form.
The switch to PostgreSQL made it possible to create a framework to write such consistency checks and run them whenever new data is added.

\bibliographystyle{alpha}
\bibliography{biblio}

\end{document}
