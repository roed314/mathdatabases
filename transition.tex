\documentclass{article}
\usepackage{amsmath,amssymb}
\usepackage{url}
\usepackage{xcolor}

\newcommand{\todo}[1]{\textcolor{red}{#1}}

\title{Maintaining Mathematical Databases}
\author{Edgar Costa and David Roe}

\begin{document}
\maketitle

\section{Introduction}

Clarify that we're talking about hosting mathematical data, rather than written mathematical content (such as Wikipedia, Mathworld, MathSciNet, arXiv) or something like the Mathematical Genealogy Project

\section{Survey of major mathematical databases}

\begin{itemize}
\item LMFDB (\url{lmfdb.org})
\item OEIS (\url{oeis.org})
\item Encyclopedia of Triangle Centers (\url{http://faculty.evansville.edu/ck6/encyclopedia/ETC.html})
\item Inverse Symbolic Calculator (\url{http://wayback.cecm.sfu.ca/projects/ISC/ISCmain.html})
\item Information System on Graph Classes and their Inclusions (\url{graphclasses.org/})
\item Complexity Zoo (\url{https://complexityzoo.uwaterloo.ca/Complexity_Zoo}) - discussion of source of data: computers or humans
\item Digital Library of Mathematical Functions (\url{https://dlmf.nist.gov/})
\item Consequences of the Axiom of Choice (\url{http://www.math.purdue.edu/~hrubin/JeanRubin/Papers/conseq.html})
\item Cantor's Attice (\url{cantorsattic.info})
\item Unipotent Muffin Research Kitchen (\url{http://lie.math.okstate.edu/UMRK/UMRK.html})
\item Atlas of Lie Groups and Representations (\url{http://www.liegroups.org/})
\item Atlas of Finite Group Representations (\url{http://brauer.maths.qmul.ac.uk/Atlas/v3/})
\item FindStat (\url{http://www.findstat.org/})
\item Matroids (\url{https://www.math.ucdavis.edu/~mkoeppe/art/Matroids/})
\item GraphArichive (\url{http://www.graph-archive.org/doku.php}) - example that died
\item Table of Knot Invariants (\url{http://www.indiana.edu/~knotinfo/})
\item Databases included in GAP/Sage/etc
\item Knot Atlas (\url{http://katlas.org/})
\item Predecessors to the LMFDB (Cremona's tables, \url{https://math.la.asu.edu/~jj/localfields/} and other Jones' databases, \url{https://wstein.org/Tables/})
\item Calabi-Yau data \url{http://hep.itp.tuwien.ac.at/~kreuzer/CY/}
\item The K3 Database \url{https://magma.maths.usyd.edu.au/magma/handbook/text/1410}
\item Fano Varieties and Extremal Laurent Polynomials \url{http://coates.ma.ic.ac.uk/fanosearch/}
\end{itemize}

See question of Tim Gowers (https://mathoverflow.net/questions/68442/what-could-be-some-potentially-useful-mathematical-databases) about what databases might be desirable to have.

\section{Hosting}

\begin{itemize}
\item On a personal webpage (easy for many academics), but limits scope (aren't running your own webserver)
\item Using a web framework (e.g. Github, Cocalc, Wordpress/wikidot/blogs)
\item On a university/personal server.  Complete control, but have to deal with down time, buying new servers, backups, security updates, DNS....
\item Servers in the cloud (e.g. Google, AWS).  Reduces hassle, but costs money.
\end{itemize}

\section{Front End}

\begin{itemize}
\item E-mail me if you want the data
\item HTML front page with downloads (static)
\item Wiki page (dynamic but human generated)
\item within computer algebra system (GAP/Sage/Magma)
\item web application (LMFDB uses Python/Flask/Jinja with javascript/CSS/etc).  Allows for dynamic generation of pages, links, nicer representation of data
\end{itemize}

\section{Back End}

\begin{itemize}
\item static files (plain text, sql-lite, Sage pickles,....)
\item Wiki software
\item document oriented database (e.g. MongoDB: schema free)
\item relational database (e.g. PostgreSQL)
\end{itemize}

\section{MongoDB vs PostgreSQL}

When the LMFDB project began, MongoDBDB was an exciting new option.  As a document-oriented database, it did not require a schema, a boon when creating new collections of mathematical objects where the data requirements only gradually became clear.  Moreover, Python bindings were available, a language that many computational number theorists were familiar with due to its use in SageMath.  It worked well for many years, but as the LMFDB grew various shortcomings of MongoDBbecame more problematic.  Over the last year, we have transitioned from MongoDB to PostgreSQL.

\subsection{Comparing two database systems}

\begin{itemize}
%\item Have to specify a schema in PostgreSQL in advance.  This can be a hassle, but also can prevent errors (e.g. typos in column names, more complicated processing code because data isn't uniform)
%\item MongoDB required more storage space since every document had to include the column names (quantify storage space change; quantify how much we're paying)
%\item Integers limited to 64 bits, so got in the habit of storing many things as strings (which made sorting tricky among other issues)
%\item Array types (not just a jsonb)
%\item MongoDB required more RAM (Wired-Tiger; MMAP). Postgres can be faster with more RAM but works okay with less.
\item Indexes on lists didn't work as we wanted (e.g. list of ramified primes) - would work for "what are all the fields ramified at 5" but not "what are all the fields ramified at 2 and 3".  Also inefficient (constructs query inefficiently since there are many more fields than primes).  Postgres has multiple index types, including gin which work well for this purpose
\item Performance improvements, important since we don't know what kind of queries the user will request so it's hard to build all indexes.  Partly because of lower storage, performance also improved even without indexes.  Non-typical queries can be the most interesting mathematically.
\item Harder to do relational queries in MongoDB; since we ported the table layout relational queries are still not used much in Postgres
\item Postgres overcame much of MongoDB's advantage with the inclusion of the json and jsonb types.
\item MongoDB was faster at counting results.  Work around this by caching counts in a table (possible since data doesn't change frequently)
\item Useful new utilities (pgAdmin 4)
\item Fast loading from a file (COPY FROM), and direct data from Magma without a Python intermediary
  add estimate
\item (TODO) Actually compare performance while we still have the mongo instance around.
\item Transactions (bugs don't cause data reliability problems) and data upload much faster
\end{itemize}

PostgreSQL is a popular open source implementation of the SQL language and grew out of earlier projects founded in the 1970s and 80s.
As an SQL database, it stores data in tables with a specified schema: each row must have the same layout, with the types of columns constant across all rows.
In contrast, the fields present in a MongoDB document can vary across a collection, as can their types.
This flexibility is convenient, but can easily lead to errors.
Multiple typos in column names were found during the transition to PostgreSQL, and differing layouts across documents requires more complicated processing code.
Moreover, the LMFDB was effectively using a schema even before the transition, to the extent that an inventory application was written to extract the schema from the data in MongoDB.
Supporting the schema at the level of the database software improves robustness and performance.

As a consequence of the shift to a schema, storing the data in PostgreSQL requires substantially less space than in MongoDB, as we do not need to store the keys for each row.
\todo{Quantify: how much storage space is used in each system?}
For example, in table with 78 columns the saving  \todo{finish sentence}
Two main factors account for this improvement.
Because the fields in a MongoDB document can vary, the names of these fields must be stored in each document.
For some of the LMFDB's collections, these names accounted for a substantial fraction of the storage requirement.
In addition to the space benefits, the switch to PostgreSQL also relaxes the pressure to minimize the lengths of these names, improving readability.

The second factor contributing to space savings is the reduced use of strings in the data itself.
As a mathematical database, the LMFDB was heavily impacted by MongoDB's lack of support for arbitrary precision integers.
In order to work around this issue, a lot of numerical data was stored as strings.
In addition to the storage consequences, various workarounds were required in order to sort data correctly.
The elimination of these hacks have simplified a lot of supporting code.
\todo{Check to see if MongoDB is less efficient than PostgreSQL when storing lists of integers; are they using a string-based json format or a compact array format?}

The two systems also have different requirements in terms of working memory.
\todo{Describe the MongoDB memory requirements}
In contrast, Postgres will work in low memory environments.
\todo{Describe the minimum memory required to run the LMFDB, as well as how additional memory impacts performance}

One of the main tools for improving search performance on databases is the use of indexes.
Indexes facilitate the location of records with constraints on a specified set of columns by storing additional data.
For example, both MongoDB and PostgreSQL support indexes based on binary trees, which work well for totally ordered data such as integers and strings.
However, there is another query type that is not well supported by binary trees.
If $K$ is a number field, the LMFDB stores a list $P_K$ of ramified primes.
We would like to be able to specify a list $Q$ of primes and be able to find all $K$ with $Q \subseteq P_K$, or all $K$ with $Q \supseteq P_K$.
MongoDB executed these searches by narrowing the results one prime at a time \todo{confirm that this is actually what MongoDB is doing}
In contrast, GIN indexes in PostgreSQL support such queries efficiently.

\subsection{Transition}

\begin{itemize}
\item Exporting data to plain text, readable using PostgreSQL' COPY FROM.
\item Choice of schema.  LMFDB's Inventory was very useful here.  Had to choose between jsonb and arrays; initially chose to just use jsonb for simplicity but more recently have used arrays for smaller storage footprint.  Tried to normalize data to some extent (away from using strings for so much). 
\item Data irregularities slowed the process down
\item Added abstraction layers.  psycopg2 is very low level (provides a way to execute SQL commands from Python and get the results back).  Because the lmfdb has almost no queries that cross multiple databases, was able to port MongoDB dictionary queries: implemented a parser that generates simple SQL.  Means that most LMFDB developers don't need to learn SQL.
\item Build data management tools into the interface (changing schema, importing new data from Python and from files).  Replaces parts of scripts.
\item Some optimizations (id ordering, split some tables in half)
\end{itemize}

\section{Benefits}

\subsection{Abstraction}

As the LMFDB has grown, the most common paradigm for adding new features has been to take a functional section of the website, copy it to a new folder, and modify the templates and backend code.
This approach has the benefit of easily producing valid code, since one is iteratively making small changes to an already functional webpage.
However, it leads to large quantities of repeated code, which make fixing bugs and adding features much harder, since the same change must be applied in dozens of locations.
The standard solution is to encapsulate tasks into functions, that can then be used repeatedly and changed at need.
Of course, the LMFDB has used functions since the beginning, but often these functions are not written in a way that allows them to be used in multiple sections of the website.

The task of switching from MongoDB to PostgreSQL was made more difficult by this low level of abstraction, but it also presented an opportunity.
Since the database change necessitated changing a large number of files throughout the project, it offered a perfect time to make additional changes.
We highlight three in particular to illustrate the kinds of encapsulation that accompanied the database change.

Each section of the LMFDB has a search page, where users can input constraints they would like their curve, field or group to satisfy.
Some input boxes require an integer or list of integers; others need a label string in a particular format; others a rational number.
The processing code must process the user provided text, raising appropriate errors on invalid input, and transform the set of inputs into a database query.
Prior to the database change, we had already begun the process of encapsulating this task.
Each input box corresponds to a specific field or column in the database, and there is a processing function for each type of input (list of integers, floating point, etc).
As we switched each part of the LMFDB to PostgreSQL, we ensured that it was using these functions.

Once these queries have been constructed, we need to execute the search and pass the results to the Jinja template that actually crafts the webpage seen by the user.
There are various parts of this task that are shared among all sections of the LMFDB.
For example, we have to be able to handle errors in user input, errors or timeouts in the execution of the query, and jump boxes that allow a user to go straight to an object with a particular label.
As part of the switch to PostgreSQL, we created a \emph{search wrapper} that performs all of these jobs in a uniform way, and adopted it across the LMFDB.
In addition to simplifying the code overall, this change made the user experience more uniform and allowed for the creation of new features, which will be described in the next section.

Many sections of the LMFDB had a statistics page prior to the database change.
These pages described the number of objects with certain attributes; for example, a table of how many number fields were available by signature, or a table giving the fraction of elliptic curves with a specified torsion structure.
Along with the database change, we created a class to support the database queries required for these statistics, and a common template to display them nicely.
This change has the benefit that it automates the process for updating these counts when developers add data.
It also make it easier to create these statistics pages for more sections of the LMFDB, and makes it easier to implement new features such as user-requested statistics.

\subsection{New Features}

In addition to intrinsic benefits of using PostgreSQL, the transition has eased the implementation of a number of new features.
While many of these would have been possible using MongoDB, the improved abstraction made implementing them much more manageable.
And some, such as dynamic statistics and verifications, rely on PostgreSQL's improved performance and relational model.

PostgreSQL supports transactions, allowing one to roll back to a functional earlier state if some later action fails.
We have built these transactions into our interface layer, and this change has enabled new paradigms when making data changes.
Rather than having to create a whole new copy of a table when making substantial changes, transactions allow sufficient peace-of-mind that more operations can be performed in-place.

The fact that all interactions with the database pass through our own interface layer allows us to add custom logging.
For example, all queries that take longer than a threshold are recorded, allowing us to focus on creating the right indexes.
We also log high-level actions, such as adding new data to a table or changing a schema, along with a username in order to provide a record of what changes have been made to the data.
These logs help us ensure the quality of our data.

The statistics infrastructure has enabled a new user interface which we refer to as \emph{dynamic statistics}.
Rather than presenting only a pre-selected set of views, the user can specify which variables they are interested in, as well as constraints on the objects.
The system the generates a display describing the possible values of those variables and the number of objects possessing each possible set of values.
For example, a user can create a table to view how the weight and level of modular forms vary among forms with complex multiplication.
We hope that this feature will provide researchers with a new method of interacting with the LMFDB, allowing them to look for large scale patterns in these objects in addition to searching for objects with particular properties.

Many browse pages in the LMFDB already had a link to view a random object.
Because of the new search wrapper abstraction, it was easy to add the ability to return a random object satisfying a search query.
This feature saves time when browsing, since it allows you to more quickly reach the homepage of an object satisfying a particular constraint, such as CM elliptic curve or a weight 1 modular form.

Mathematical data differs from data in most other applications because it has a notion of correctness that can be checked, rather than just providing a record of a real-world measurement.
Some of these verifications are internal, such as checking that the defining polynomial of a number field is irreducible or that the trace of a modular form is zero at each inert prime not dividing the level.
Others rely on connections with other tables in the database, such as checking that invariants match up between an elliptic curve and its corresponding modular form.
The switch to PostgreSQL made it possible to create a framework to write such consistency checks and run them whenever new data is added.

\end{document}
